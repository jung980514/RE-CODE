"use client"

export const dynamic = 'force-dynamic'

import { useState, useRef, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Card } from "@/components/ui/card"
import { Play, RotateCcw, Mic, ChevronRight, Camera, Pause } from "lucide-react"
import TrainingCompleteModal from "@/app/main-elder/recall-training/components/TrainingCompleteModal"
import { useRouter } from "next/navigation"
import { synthesizeSpeech, playAudio, stopCurrentAudio } from "@/api/googleTTS/googleTTSService"
import { markRecallTrainingSessionAsCompleted } from "@/lib/auth"
import { FloatingButtons } from "@/components/common/Floting-Buttons"

// ê°ì • ë¶„ì„ í›…
interface EmotionRecord {
  timestamp: number;
  emotion: string;
  confidence: number;
}

type FaceExpressions = {
  neutral: number
  happy: number
  sad: number
  angry: number
  fearful: number
  disgusted: number
  surprised: number
}

type FaceApiModule = {
  nets: {
    tinyFaceDetector: { loadFromUri: (uri: string) => Promise<void> }
    faceExpressionNet: { loadFromUri: (uri: string) => Promise<void> }
  }
  TinyFaceDetectorOptions: new () => object
  detectSingleFace: (
    input: HTMLVideoElement,
    options: object
  ) => { withFaceExpressions: () => Promise<{ expressions: FaceExpressions } | null> }
}

function useEmotionDetection(
  videoRef: React.RefObject<HTMLVideoElement | null>,
  onSecondSample?: (sample: { timestamp: number; emotion: string; confidence: number }) => void,
) {
  // ì„œë²„ ë²ˆë“¤ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë™ì  ì„í¬íŠ¸ë¡œë§Œ ì‚¬ìš©
  const faceapiRef = useRef<FaceApiModule | null>(null)
  const [emotion, setEmotion] = useState<string>('ì¤‘ë¦½')
  const [confidence, setConfidence] = useState<number>(0)
  const requestRef = useRef<number | undefined>(undefined)
  const modelsLoaded = useRef<boolean>(false)
  const prevExpressions = useRef<FaceExpressions | null>(null)
  const emotionHistory = useRef<EmotionRecord[]>([])
  const lastRecordTime = useRef<number>(0)

  useEffect(() => {
    const loadModels = async () => {
      try {
        // í´ë¼ì´ì–¸íŠ¸ì—ì„œë§Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
        if (!faceapiRef.current) {
          const faceModule = (await import('@vladmandic/face-api')) as unknown as FaceApiModule
          faceapiRef.current = faceModule
        }
        // CDNì—ì„œ ëª¨ë¸ ë¡œë“œ
        await Promise.all([
          faceapiRef.current.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model'),
          faceapiRef.current.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model')
        ])
        modelsLoaded.current = true
        console.log('ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')
      } catch (error) {
        console.error('ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜:', error)
      }
    }
    loadModels()

    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [])

  const detectEmotion = async () => {
    if (!modelsLoaded.current || !videoRef.current || !faceapiRef.current) return

    try {
      const detections = await faceapiRef.current
        .detectSingleFace(videoRef.current, new faceapiRef.current.TinyFaceDetectorOptions())
        .withFaceExpressions()

      if (detections) {
        const expressions = detections.expressions as FaceExpressions
        
        // ê°ì • ì ìˆ˜ ì¡°ì •
        const emotionChange = prevExpressions.current ? 
          Math.abs(expressions.neutral - prevExpressions.current.neutral) : 0

        // ë§í•˜ê¸°/í‘œì • ë³€í™” ê°ì§€ ë¡œì§
        const isSpeaking = emotionChange > 0.15
        const isSmiling = expressions.happy > 0.3
        const isNeutralDominant = expressions.neutral > 0.5
        
        // ìŠ¬í”” ê°ì • ë³´ì •
        const sadScore = expressions.sad
        const mouthOpenScore = Math.max(expressions.surprised, expressions.sad)
        const isMouthOpen = mouthOpenScore > 0.3
        
        // ì‹¤ì œ ìŠ¬í”” í‘œì • íŒë‹¨
        const isActuallySad = sadScore > 0.4 && !isSpeaking && !isSmiling && !isMouthOpen
        
        const adjustedExpressions: FaceExpressions = {
          ...expressions,
          surprised: expressions.surprised * (isSpeaking ? 0.3 : 0.7),
          sad: expressions.sad * (isActuallySad ? 1.0 : 0.1),
          neutral: expressions.neutral * (isNeutralDominant ? 1.4 : 1.2),
          happy: expressions.happy * (isSmiling ? 1.5 : 1.2),
          angry: expressions.angry * (isSpeaking ? 0.7 : 0.9),
          fearful: expressions.fearful * (isSpeaking ? 0.7 : 0.9),
          disgusted: expressions.disgusted * 0.8
        }

        prevExpressions.current = expressions

        let maxExpression = 'neutral'
        let maxConfidence = adjustedExpressions.neutral
        const threshold = 0.3

        // ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê°ì • ì°¾ê¸°
        ;(Object.keys(adjustedExpressions) as Array<keyof FaceExpressions>).forEach((expression) => {
          const value = adjustedExpressions[expression]
          let currentThreshold = threshold
          if (expression === 'surprised') currentThreshold = threshold * 1.5
          if (expression === 'sad') currentThreshold = threshold * 1.4
          
          if (value > maxConfidence && value > currentThreshold) {
            if (isSpeaking && (expression === 'neutral' || expression === 'happy')) {
              maxConfidence = value * 1.2
            } else {
              maxConfidence = value
            }
            maxExpression = expression as string
          }
        })

        // ê°ì • í•œê¸€í™”
        const emotionMap: { [key: string]: string } = {
          neutral: 'ì¤‘ë¦½',
          happy: 'í–‰ë³µ',
          sad: 'ìŠ¬í””',
          angry: 'í™”ë‚¨',
          fearful: 'ë‘ë ¤ì›€',
          disgusted: 'í˜ì˜¤',
          surprised: 'ë†€ëŒ'
        }

        const newEmotion = emotionMap[maxExpression] || 'ì¤‘ë¦½'
        if (maxConfidence > threshold) {
          setEmotion(newEmotion)
          setConfidence(Math.round(maxConfidence * 100))

          // 1ì´ˆë§ˆë‹¤ ê°ì • ìƒíƒœ ê¸°ë¡
          const now = Date.now()
          if (now - lastRecordTime.current >= 1000) {
            const sample = {
              timestamp: now,
              emotion: newEmotion,
              confidence: maxConfidence
            }
            emotionHistory.current.push(sample)
            if (typeof onSecondSample === 'function') {
              onSecondSample(sample)
            }
            lastRecordTime.current = now
          }
        }
      }
    } catch (error) {
      console.error('ê°ì • ë¶„ì„ ì˜¤ë¥˜:', error)
    }

    requestRef.current = requestAnimationFrame(detectEmotion)
  }

  useEffect(() => {
    if (videoRef.current && videoRef.current.videoWidth > 0) {
      detectEmotion()
    }

    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [videoRef.current?.videoWidth])

  return { emotion, confidence }
}

export function VoiceMusicTherapySession({ onBack }: { onBack: () => void }) {
  const router = useRouter()
  const [isRecording, setIsRecording] = useState(false)
  const [isPlaying, setIsPlaying] = useState(false)
  const [cameraLoading, setCameraLoading] = useState(true)
  const [cameraError, setCameraError] = useState(false)
  const [questions, setQuestions] = useState<Array<{ questionId: number; content: string; mediaUrl: string; mediaType: string; createdAt: string }>>([])
  const [currentIndex, setCurrentIndex] = useState<number>(0)
  const [questionsLoading, setQuestionsLoading] = useState<boolean>(false)
  const [questionsError, setQuestionsError] = useState<string | null>(null)
  const [isUploading, setIsUploading] = useState<boolean>(false)
  const [hasRecorded, setHasRecorded] = useState<boolean>(false)
  const [showCompleteModal, setShowCompleteModal] = useState<boolean>(false)
  const [finalEmotion, setFinalEmotion] = useState<string>('NEUTRAL')

  const videoRef = useRef<HTMLVideoElement>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const recordedBlobRef = useRef<Blob | null>(null)
  const selectedMimeTypeRef = useRef<string>('video/mp4')
  const selectedExtensionRef = useRef<string>('mp4')
  const sessionEmotionHistory = useRef<Array<{ timestamp: number; emotion: string; confidence: number }>>([])
  const [sessionActive, setSessionActive] = useState<boolean>(false)
  
  // GIF ì´ë¯¸ì§€ ê´€ë ¨ ìƒíƒœ
  const [isSpeaking, setIsSpeaking] = useState(false)
  const audioContextGifRef = useRef<AudioContext | null>(null)
  const analyserGifRef = useRef<AnalyserNode | null>(null)
  const micStreamGifRef = useRef<MediaStream | null>(null)
  const rafGifRef = useRef<number | null>(null)
  const lastAboveThresholdMsGifRef = useRef<number>(0)
  
  // ë§í’ì„  ëœë¤ ì´ë¯¸ì§€ ìƒíƒœ
  const [currentBalloonImage, setCurrentBalloonImage] = useState<string>('')

  // ëœë¤ ë§í’ì„  ì´ë¯¸ì§€ ì„ íƒ í•¨ìˆ˜
  const getRandomBalloonImage = (isSpeaking: boolean): string => {
    const randomNumber = Math.floor(Math.random() * 5) + 1 // 1~5 ëœë¤
    const folder = isSpeaking ? 'talk' : 'nottalk'
    return `/images/talkballoon/${folder}/${randomNumber}.png`
  }

  // isSpeaking ìƒíƒœ ë³€ê²½ ì‹œ ëœë¤ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
  useEffect(() => {
    setCurrentBalloonImage(getRandomBalloonImage(isSpeaking))
  }, [isSpeaking])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ì´ˆê¸° ì´ë¯¸ì§€ ì„¤ì •
  useEffect(() => {
    setCurrentBalloonImage(getRandomBalloonImage(false))
  }, [])
  
  // ê°ì • ë¶„ì„ í›… ì‚¬ìš©
  const { emotion, confidence } = useEmotionDetection(
    videoRef,
    (sample) => {
      // ì„¸ì…˜ ê¸°ë¡ì€ 1ì´ˆ ë‹¨ìœ„ ìƒ˜í”Œë§Œ ì €ì¥
      if (sessionActive) {
        sessionEmotionHistory.current.push(sample)
      }
    }
  )

  // ìŒì„± ê°ì§€ ì´ˆê¸°í™” (GIF ì´ë¯¸ì§€ìš©)
  useEffect(() => {
    let cancelled = false
    const initAudioDetection = async () => {
      try {
        const micStream = await navigator.mediaDevices.getUserMedia({ audio: true })
        if (cancelled) {
          micStream.getTracks().forEach(t => t.stop())
          return
        }
        micStreamGifRef.current = micStream
        
        // í˜¸í™˜: í‘œì¤€ AudioContext ìš°ì„ , ì—†ìœ¼ë©´ webkitAudioContext ì‚¬ìš©
        let audioContext: AudioContext
        if ('AudioContext' in window) {
          const Ctor = window.AudioContext as {
            new (contextOptions?: AudioContextOptions): AudioContext
          }
          audioContext = new Ctor()
        } else if ('webkitAudioContext' in window) {
          const Ctor = (window as unknown as {
            webkitAudioContext: { new (contextOptions?: AudioContextOptions): AudioContext }
          }).webkitAudioContext
          audioContext = new Ctor()
        } else {
          throw new Error('Web Audio API not supported')
        }
        audioContextGifRef.current = audioContext
        const source = audioContext.createMediaStreamSource(micStream)
        const analyser = audioContext.createAnalyser()
        analyser.fftSize = 2048
        analyser.smoothingTimeConstant = 0.8
        analyserGifRef.current = analyser
        source.connect(analyser)

        const data = new Uint8Array(analyser.frequencyBinCount)
        const SPEAKING_THRESHOLD = 12 // ì„ê³„ê°’(ì¡°ì • ê°€ëŠ¥)
        const SILENCE_HOLD_MS = 3000  // ë¬´ìŒ 3ì´ˆ ìœ ì§€ ì‹œì—ë§Œ ë¹„ë°œí™”ë¡œ ì „í™˜

        const loop = () => {
          if (!analyserGifRef.current) return
          analyserGifRef.current.getByteTimeDomainData(data)
          // 128 ê¸°ì¤€ìœ¼ë¡œ í¸ì°¨ RMS ê³„ì‚°
          let sumSquares = 0
          for (let i = 0; i < data.length; i++) {
            const v = data[i] - 128
            sumSquares += v * v
          }
          const rms = Math.sqrt(sumSquares / data.length)
          const isNowSpeaking = rms > SPEAKING_THRESHOLD
          const now = performance.now()
          if (isNowSpeaking) {
            // ë°œí™” ê°ì§€: ì¦‰ì‹œ speaking ì „í™˜, íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹ 
            if (!isSpeaking) {
              setIsSpeaking(true)
            }
            lastAboveThresholdMsGifRef.current = now
          } else {
            // ë¬´ìŒ: ë§ˆì§€ë§‰ ë°œí™” ì‹œì ìœ¼ë¡œë¶€í„° 3ì´ˆ ê²½ê³¼ ì‹œì—ë§Œ speaking í•´ì œ
            if (isSpeaking) {
              if (now - lastAboveThresholdMsGifRef.current >= SILENCE_HOLD_MS) {
                setIsSpeaking(false)
              }
            }
          }
          rafGifRef.current = requestAnimationFrame(loop)
        }
        rafGifRef.current = requestAnimationFrame(loop)
      } catch (e) {
        console.warn('ë§ˆì´í¬ ì ‘ê·¼/ë¶„ì„ ì´ˆê¸°í™” ì‹¤íŒ¨:', e)
      }
    }

    initAudioDetection()
    return () => {
      cancelled = true
      if (rafGifRef.current) {
        cancelAnimationFrame(rafGifRef.current)
        rafGifRef.current = null
      }
      if (analyserGifRef.current) {
        try { analyserGifRef.current.disconnect() } catch {}
        analyserGifRef.current = null
      }
      if (audioContextGifRef.current) {
        try { audioContextGifRef.current.close() } catch {}
        audioContextGifRef.current = null
      }
      if (micStreamGifRef.current) {
        try { micStreamGifRef.current.getTracks().forEach(t => t.stop()) } catch {}
        micStreamGifRef.current = null
      }
    }
  }, [isSpeaking])

  useEffect(() => {
    initializeCamera()
    const fetchQuestions = async () => {
      try {
        setQuestionsLoading(true)
        setQuestionsError(null)
        const res = await fetch(`${process.env.NEXT_PUBLIC_BACKEND_URL}/api/cognitive/questions/audio`, {
          method: 'GET',
          credentials: 'include',
        })
        if (!res.ok) throw new Error(`HTTP ${res.status}`)
        const json = await res.json()
        const list: Array<{ questionId: number; content: string; mediaUrl: string; mediaType: string; createdAt: string }> =
          Array.isArray(json?.data) ? json.data : []
        setQuestions(list)
        setCurrentIndex(0)
        setHasRecorded(false)
      } catch (e) {
        setQuestionsError('ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } finally {
        setQuestionsLoading(false)
      }
    }
    // eslint-disable-next-line @typescript-eslint/no-floating-promises
    fetchQuestions()

    return () => {
      // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ëª¨ë“  ì˜¤ë””ì˜¤ ì¤‘ì§€
      stopCurrentAudio()
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop())
      }
      if (audioRef.current) {
        audioRef.current.pause()
        audioRef.current.currentTime = 0
      }
    }
  }, [])

  const initializeCamera = async () => {
    try {
      setCameraLoading(true)
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: "user",
        },
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100,
        },
      })

      if (videoRef.current) {
        videoRef.current.srcObject = stream
        streamRef.current = stream
      }

      setCameraLoading(false)
      setCameraError(false)
    } catch (error) {
      console.error("Camera access failed:", error)
      setCameraLoading(false)
      setCameraError(true)
    }
  }

  const startRecording = async () => {
    if (!streamRef.current) return

    try {
      // ë…¹í™” ì‹œì‘ ì‹œ TTSì™€ ìŒì•… ëª¨ë‘ ì¤‘ì§€
      stopCurrentAudio()
      const el = audioRef.current
      if (el) {
        el.pause()
        setIsPlaying(false)
      }
      const videoTracks = streamRef.current.getVideoTracks()
      const audioTracks = streamRef.current.getAudioTracks()

      if (videoTracks.length === 0) {
        console.error("No video track available")
        return
      }

      if (audioTracks.length === 0) {
        console.warn("No audio track available - recording video only")
      } else {
        console.log("Audio tracks found:", audioTracks.length)
      }

      let mimeType = "video/webm"
      let fileExtension = "webm"

      const mp4Types = [
        'video/mp4; codecs="avc1.424028, mp4a.40.2"', // H.264 + AAC (most compatible)
        'video/mp4; codecs="avc1.42E01E, mp4a.40.2"', // H.264 baseline + AAC
        "video/mp4", // Generic MP4
      ]

      const webmTypes = [
        'video/webm; codecs="vp9, opus"', // VP9 + Opus (good quality)
        'video/webm; codecs="vp8, opus"', // VP8 + Opus (more compatible)
        "video/webm", // Generic WebM
      ]

      for (const type of mp4Types) {
        if (MediaRecorder.isTypeSupported(type)) {
          mimeType = type
          fileExtension = "mp4"
          console.log("Using MP4 format:", type)
          break
        }
      }

      if (fileExtension === "webm") {
        for (const type of webmTypes) {
          if (MediaRecorder.isTypeSupported(type)) {
            mimeType = type
            fileExtension = "webm"
            console.log("Using WebM format:", type)
            break
          }
        }
      }

      const mediaRecorder = new MediaRecorder(streamRef.current, {
        mimeType,
        audioBitsPerSecond: 128000,
        videoBitsPerSecond: 2500000,
      })

      const chunks: BlobPart[] = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data)
          console.log("Data chunk received:", event.data.size, "bytes")
        }
      }

      mediaRecorder.onstop = () => {
        console.log("Recording stopped, creating blob with", chunks.length, "chunks")
        const blob = new Blob(chunks, { type: mimeType })
        console.log("Final blob size:", blob.size, "bytes, type:", blob.type)
        // ë‹¤ìš´ë¡œë“œ ëŒ€ì‹  ë©”ëª¨ë¦¬ì— ë³´ê´€í•˜ì—¬ ì œì¶œì— ì‚¬ìš©
        recordedBlobRef.current = blob
        selectedMimeTypeRef.current = mimeType
        selectedExtensionRef.current = fileExtension
        setHasRecorded(true)
      }

      mediaRecorder.onerror = (event) => {
        console.error("MediaRecorder error:", event)
      }

      mediaRecorder.start(1000)
      mediaRecorderRef.current = mediaRecorder
      setIsRecording(true)
      setSessionActive(true)
      setHasRecorded(false)
      console.log("Recording started with format:", mimeType)
    } catch (error) {
      console.error("Recording failed:", error)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      setSessionActive(false)
    }
  }

  const handleAnswerClick = () => {
    if (isRecording) {
      stopRecording()
    } else {
      startRecording()
    }
  }

  const toggleAudioPlay = async () => {
    const el = audioRef.current
    if (!el) return
    try {
      if (el.paused) {
        await el.play()
        setIsPlaying(true)
      } else {
        el.pause()
        setIsPlaying(false)
      }
    } catch (e) {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', e)
    }
  }

  const replayAudio = async () => {
    try {
      // ìŒì•…ê³¼ TTS í•¨ê»˜ ë‹¤ì‹œ ì¬ìƒ
      await playMusicAndTTS()
    } catch (e) {
      console.error('ì˜¤ë””ì˜¤ ë‹¤ì‹œì¬ìƒ ì˜¤ë¥˜:', e)
    }
  }

  // s3:// URLì„ ë¸Œë¼ìš°ì €ì—ì„œ ì¬ìƒ ê°€ëŠ¥í•œ URLë¡œ ë³€í™˜
  const resolveMediaUrl = (rawUrl: string): string => {
    if (!rawUrl) return ''
    if (rawUrl.startsWith('http://') || rawUrl.startsWith('https://')) return rawUrl
    const DEFAULT_REGION = 'ap-northeast-2'
    const REGION = process.env.NEXT_PUBLIC_S3_REGION || DEFAULT_REGION
    const PUBLIC_DOMAIN = process.env.NEXT_PUBLIC_S3_PUBLIC_DOMAIN // ì˜ˆ: dxxxxx.cloudfront.net
    const match = rawUrl.match(/^s3:\/\/([^\/]+)\/(.+)$/)
    if (!match) return rawUrl
    const bucket = match[1]
    const key = encodeURIComponent(match[2]).replace(/%2F/g, '/')
    if (PUBLIC_DOMAIN && PUBLIC_DOMAIN.trim().length > 0) {
      return `https://${PUBLIC_DOMAIN}/${key}`
    }
    return `https://${bucket}.s3.${REGION}.amazonaws.com/${key}`
  }

  // ìë™ ìŒì•… ì¬ìƒ ë° TTS í•¨ìˆ˜
  const playMusicAndTTS = async () => {
    if (questions.length === 0 || questionsLoading) return
    
    try {
      // ê¸°ì¡´ ì˜¤ë””ì˜¤ì™€ TTS ì¤‘ì§€
      stopCurrentAudio()
      const el = audioRef.current
      if (el) {
        el.pause()
        el.currentTime = 0
      }
      
      // ìŒì•… ìë™ ì¬ìƒ
      if (el && questions[currentIndex]?.mediaUrl) {
        await el.play()
        setIsPlaying(true)
      }
      
      // TTS ì¬ìƒ (ìŒì•…ê³¼ ë™ì‹œì—)
      const question = questions[currentIndex]?.content || ''
      const ttsText = `${question}\nì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ ë‹µë³€í•˜ê¸°ë¥¼ ëˆŒëŸ¬ ë§ì”€í•´ì£¼ì„¸ìš”.`
      const audioContent = await synthesizeSpeech(ttsText)
      await playAudio(audioContent)
    } catch (error) {
      console.error('ìŒì•… ë˜ëŠ” TTS ì¬ìƒ ì‹¤íŒ¨:', error)
    }
  }

  // ì§ˆë¬¸ ì „í™˜ ì‹œ ì˜¤ë””ì˜¤ ìƒíƒœ ì´ˆê¸°í™” ë° ìë™ ì¬ìƒ
  useEffect(() => {
    const el = audioRef.current
    if (!el) return
    el.pause()
    el.currentTime = 0
    setIsPlaying(false)
    setHasRecorded(false)
    
    // ì§ˆë¬¸ì´ ë¡œë“œëœ í›„ ìë™ ì¬ìƒ
    if (!questionsLoading && questions.length > 0) {
      // ì•½ê°„ì˜ ì§€ì—° í›„ ì¬ìƒ (DOM ì—…ë°ì´íŠ¸ ëŒ€ê¸°)
      setTimeout(() => {
        playMusicAndTTS()
      }, 500)
    }
  }, [currentIndex, questionsLoading, questions.length])

  // í˜„ì¬ ì§ˆë¬¸ ë‹µë³€ ì—…ë¡œë“œ
  const uploadCurrentAnswer = async (): Promise<void> => {
    try {
      if (!recordedBlobRef.current) {
        console.warn('ì—…ë¡œë“œí•  ë…¹í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤')
        return
      }
      if (questions.length === 0) return
      const question = questions[currentIndex]
      const userId = (typeof window !== 'undefined' ? localStorage.getItem('userId') : null) || ''
      const formData = new FormData()
      formData.append('userId', userId)
      formData.append('questionId', String(question.questionId))
      formData.append('mediaType', 'audio')
      const filename = `answer-${question.questionId}.${selectedExtensionRef.current}`
      const file = new File([recordedBlobRef.current], filename, { type: selectedMimeTypeRef.current })
      formData.append('videoFile', file)

      setIsUploading(true)
              const res = await fetch(`${process.env.NEXT_PUBLIC_BACKEND_URL}/api/cognitive/answers`, {
        method: 'POST',
        credentials: 'include',
        body: formData,
      })
      if (!res.ok) {
        throw new Error(`ì—…ë¡œë“œ ì‹¤íŒ¨: HTTP ${res.status}`)
      }
      // ì—…ë¡œë“œ ì„±ê³µ í›„ ë³´ê´€ ë°ì´í„° í•´ì œ
      recordedBlobRef.current = null
    } catch (e) {
      console.error('ë‹µë³€ ì—…ë¡œë“œ ì—ëŸ¬:', e)
    } finally {
      setIsUploading(false)
    }
  }

  const handleNext = () => {
    if (questions.length === 0) return
    
    // ê¸°ì¡´ ì˜¤ë””ì˜¤ì™€ TTS ì¤‘ì§€
    stopCurrentAudio()
    const el = audioRef.current
    if (el) {
      el.pause()
      el.currentTime = 0
    }
    if (isRecording) {
      stopRecording()
    }
    setIsPlaying(false)
    
    // ì—…ë¡œë“œ í›„ ë‹¤ìŒ ë˜ëŠ” ì™„ë£Œ ì²˜ë¦¬
    void (async () => {
      await uploadCurrentAnswer()
      const isLast = currentIndex === (questions.length - 1)
      if (isLast) {
        // ì„¸ì…˜ ê°ì • ìš”ì•½ ê³„ì‚°
        try {
          const records = sessionEmotionHistory.current
          if (records.length > 0) {
            const startTs = records[0].timestamp
            const endTs = records[records.length - 1].timestamp
            const totalDurationSec = Math.max(1, Math.round((endTs - startTs) / 1000))
            const durationPerEmotion: Record<string, number> = {}
            for (let i = 0; i < records.length; i += 1) {
              const emo = records[i].emotion
              durationPerEmotion[emo] = (durationPerEmotion[emo] || 0) + 1
            }
            const thresholdSec = Math.ceil(totalDurationSec * 0.17)
            type Dominant = { emo: string; sec: number }
            let dominant: Dominant | null = null
            Object.entries(durationPerEmotion).forEach(([emo, sec]) => {
              if (emo !== 'ì¤‘ë¦½' && sec >= thresholdSec) {
                if (!dominant || sec > dominant.sec) dominant = { emo, sec }
              }
            })
            if (!dominant) {
              Object.entries(durationPerEmotion).forEach(([emo, sec]) => {
                if (!dominant || sec > dominant.sec) dominant = { emo, sec }
              })
            }
            const koToEn: Record<string, string> = {
              'ì¤‘ë¦½': 'NEUTRAL',
              'í–‰ë³µ': 'HAPPY',
              'ìŠ¬í””': 'SAD',
              'í™”ë‚¨': 'ANGRY',
              'ë‘ë ¤ì›€': 'FEAR',
              'í˜ì˜¤': 'DISGUST',
              'ë†€ëŒ': 'SURPRISED',
            }
            const chosen = dominant as { emo: string; sec: number } | null
            const emotionCode = (chosen ? (koToEn[chosen.emo] || 'NEUTRAL') : 'NEUTRAL').toUpperCase()
            setFinalEmotion(emotionCode)
            // ê¸°ë¡ ì´ˆê¸°í™”
            sessionEmotionHistory.current = []
          } else {
            setFinalEmotion('NEUTRAL')
          }
        } catch (e) {
          console.error('ì„¸ì…˜ ê°ì • ìš”ì•½ ê³„ì‚° ì˜¤ë¥˜:', e)
          setFinalEmotion('NEUTRAL')
        }
        setShowCompleteModal(true)
        return
      }
      setCurrentIndex((prev) => (prev + 1) % questions.length)
      setHasRecorded(false)
    })()
  }

  // ê°ì •ë³„ ìƒ‰ìƒ ë° ì´ëª¨ì§€ ë°˜í™˜ í•¨ìˆ˜
  const getEmotionColor = (emotion: string) => {
    switch (emotion) {
      case 'í–‰ë³µ':
        return 'text-yellow-600'
      case 'ìŠ¬í””':
        return 'text-blue-600'
      case 'í™”ë‚¨':
        return 'text-red-600'
      case 'ë‘ë ¤ì›€':
        return 'text-purple-600'
      case 'í˜ì˜¤':
        return 'text-green-700'
      case 'ë†€ëŒ':
        return 'text-orange-600'
      case 'ì¤‘ë¦½':
      default:
        return 'text-emerald-600'
    }
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-emerald-50 to-teal-50 p-4 relative">


      {/* Header */}
      <div className="max-w-6xl mx-auto mb-8">

        <div>
          <h1 className="text-4xl font-bold text-gray-800 mb-2" style={{ fontFamily: 'Pretendard' }}>ë“¤ë ¤ì˜¤ëŠ” ì¶”ì–µ í›ˆë ¨</h1>
        </div>
      </div>

              {/* Main Content */}
        <div className="max-w-6xl mx-auto grid lg:grid-cols-3 gap-6">
          {/* Left Panel - Audio Exercise */}
                     <Card className="lg:col-span-2 p-5 md:p-6 bg-white shadow-2xl rounded-2xl min-h-[500px]">
          <div className="text-center mb-4">
            <div className="inline-flex items-center gap-2 bg-emerald-100 text-emerald-700 px-4 py-2 rounded-full text-lg font-medium mb-3" style={{ fontFamily: 'Pretendard' }}>
              <span>ğŸµ</span>
              {questionsLoading ? (
                <span>ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...</span>
              ) : (
                <span>
                 ì†Œë¦¬ {questions.length > 0 ? currentIndex + 1 : 0}/{questions.length}
                </span>
              )}
            </div>

            <h2 className="text-2xl font-bold text-emerald-600 mb-2" style={{ fontFamily: 'Pretendard' }}>ì¸ì§€ìê·¹í›ˆë ¨ (ì†Œë¦¬)</h2>
          </div>

                      {/* Audio Player */}
            <div className="flex items-center justify-between bg-gray-50 rounded-2xl p-4 mb-5">
            <div className="flex items-center gap-4">
              <div className="w-12 h-12 bg-emerald-500 rounded-full flex items-center justify-center">
                <span className="text-white font-bold text-sm">ğŸµ</span>
              </div>
              <div>
                <h3 className="font-semibold text-gray-800 text-lg" style={{ fontFamily: 'Pretendard' }}>ì¸ì§€ìê·¹í›ˆë ¨</h3>
                <p className="text-emerald-600 text-base" style={{ fontFamily: 'Pretendard' }}>ì†Œë¦¬</p>
              </div>
            </div>

            <Button onClick={toggleAudioPlay} disabled={questionsLoading || questions.length === 0} className="w-12 h-12 rounded-full bg-emerald-500 hover:bg-emerald-600 disabled:opacity-60 disabled:cursor-not-allowed">
              {isPlaying ? (
                <Pause className="w-5 h-5 text-white" />
              ) : (
                <Play className="w-5 h-5 text-white" />
              )}
            </Button>
          </div>

          {/* Hidden/inline audio element */}
          <audio
            ref={audioRef}
            src={questions.length > 0 ? resolveMediaUrl(questions[currentIndex].mediaUrl) : undefined}
            onEnded={() => setIsPlaying(false)}
            preload="none"
          />

                      {/* Question */}
            <div className="mb-5">
            <div className="flex items-start gap-3 mb-4">
              <div>
                {questionsLoading ? (
                  <p className="text-gray-600 text-xl" style={{ fontFamily: 'Pretendard' }}>ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...</p>
                ) : questionsError ? (
                  <p className="text-red-600 text-xl" style={{ fontFamily: 'Pretendard' }}>{questionsError}</p>
                ) : questions.length > 0 ? (
                  <div className="text-gray-900 leading-relaxed text-3xl space-y-4" style={{ fontFamily: 'Paperlogy' }}>
                    <p className="mb-4">
                      {questions[currentIndex].content}
                    </p>
                    <p className="text-xl text-gray-700">
                      ì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ <strong className="text-emerald-700">ë‹µë³€í•˜ê¸°</strong>ë¥¼ ëˆŒëŸ¬ ë§ì”€í•´ì£¼ì„¸ìš”
                    </p>
                  </div>
                ) : (
                  <p className="text-gray-600 text-xl" style={{ fontFamily: 'Pretendard' }}>í‘œì‹œí•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.</p>
                )}
              </div>
            </div>
          </div>

          {/* Control Buttons */}
          <div className="flex gap-6">
            <Button
              variant="outline"
              className="flex-1 h-16 text-2xl border-2 border-emerald-400 text-emerald-800 hover:bg-emerald-50 bg-white focus-visible:ring-4 focus-visible:ring-emerald-300 rounded-xl disabled:opacity-60 disabled:cursor-not-allowed"
              style={{ fontFamily: 'Pretendard' }}
              onClick={replayAudio}
              disabled={questionsLoading || questions.length === 0}
              aria-label="ìŒì•… ë‹¤ì‹œ ì¬ìƒ"
            >
              ë‹¤ì‹œ ì¬ìƒ
            </Button>

            <Button
              className={`flex-1 h-16 text-2xl text-white focus-visible:ring-4 focus-visible:ring-emerald-300 rounded-xl ${
                isRecording ? 'bg-red-600 hover:bg-red-700' : 'bg-green-600 hover:bg-green-700'
              }`}
              style={{ fontFamily: 'Pretendard' }}
              onClick={handleAnswerClick}
              aria-pressed={isRecording}
              aria-label={isRecording ? 'ë…¹í™”ì¤‘ì§€' : (hasRecorded ? 'ë‹¤ì‹œë‹µë³€' : 'ë‹µë³€í•˜ê¸°')}
            >
              {isRecording ? 'ë…¹í™”ì¤‘ì§€' : (hasRecorded ? 'ë‹¤ì‹œë‹µë³€' : 'ë‹µë³€í•˜ê¸°')}
            </Button>

            <Button
              className={`flex-1 h-16 text-2xl text-white focus-visible:ring-4 focus-visible:ring-emerald-300 disabled:opacity-60 disabled:cursor-not-allowed rounded-xl min-w-[150px] flex items-center justify-center gap-3 ${
                isUploading 
                  ? 'bg-emerald-600 cursor-wait' 
                  : 'bg-emerald-700 hover:bg-emerald-800'
              }`}
              style={{ fontFamily: 'Pretendard' }}
              onClick={handleNext}
              disabled={questions.length === 0 || isRecording || !hasRecorded || isUploading}
              aria-label={questions.length > 0 && currentIndex === questions.length - 1 ? 'ì™„ë£Œí•˜ê¸°' : 'ë‹¤ìŒ ìŒì„±ìœ¼ë¡œ ì´ë™'}
            >
              {isUploading ? (
                <>
                  <div className="animate-spin rounded-full h-5 w-5 border-2 border-white border-t-transparent"></div>
                  <span>ë¡œë”© ì¤‘</span>
                </>
              ) : (
                questions.length > 0 && currentIndex === questions.length - 1 ? 'ì™„ë£Œí•˜ê¸°' : 'ë‹¤ìŒ ìŒì„±'
              )}
            </Button>
          </div>
        </Card>

                  {/* Right Panel - Webcam */}
                     <Card className="lg:col-span-1 p-5 md:p-6 bg-white shadow-2xl rounded-2xl min-h-[400px]" aria-label="ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°">
            <h3 className="text-2xl text-center font-extrabold text-gray-900 mb-4" style={{ fontFamily: 'Pretendard' }}>ë‚´ í™”ë©´</h3>

          {/* Webcam Display */}
          <div className="relative bg-gray-900 rounded-2xl overflow-hidden mb-4" style={{ aspectRatio: "4/3" }}>
            {cameraLoading && (
              <div className="absolute inset-0 flex flex-col items-center justify-center text-gray-400">
                <Camera className="w-16 h-16 mb-4 opacity-50" />
                <p className="text-sm">ì¹´ë©”ë¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...</p>
              </div>
            )}

            {cameraError && (
              <div className="absolute inset-0 flex flex-col items-center justify-center text-gray-400">
                <Camera className="w-16 h-16 mb-4 opacity-50" />
                <p className="text-sm">ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨</p>
                <Button variant="outline" size="sm" className="mt-2 bg-transparent" onClick={initializeCamera}>
                  ë‹¤ì‹œ ì‹œë„
                </Button>
              </div>
            )}

            <video
              ref={videoRef}
              autoPlay
              muted
              playsInline
              className={`w-full h-full object-cover ${cameraLoading || cameraError ? "hidden" : ""}`}
            />

            {isRecording && (
              <div className="absolute top-4 right-4 flex items-center gap-2 bg-red-500 text-white px-3 py-1 rounded-full text-sm">
                <div className="w-2 h-2 bg-white rounded-full animate-pulse" />
                ë…¹í™”ì¤‘
              </div>
            )}
          </div>

                      {/* Status */}
            <div className="space-y-4 mb-4">
              <div className="flex justify-between items-center text-xl">
                <span className="text-gray-800" style={{ fontFamily: 'Pretendard' }}>í˜„ì¬ ê°ì •</span>
                <span className="text-emerald-700 font-extrabold" style={{ fontFamily: 'Pretendard' }}>{emotion}</span>
              </div>
            </div>

            {/* Image Area */}
            <div className="flex-1">
              <div className="h-full bg-white rounded-2xl overflow-visible relative min-h-[200px]">
                {/* ìŒì„± ê°ì§€ì— ë”°ë¥¸ GIF ì´ë¯¸ì§€ í‘œì‹œ */}
                <div className="flex items-center justify-center h-full">
                  <img
                    src={isSpeaking ? "/images/hearfox.gif" : "/images/speepfox.gif"}
                    alt={isSpeaking ? "ë§í•˜ëŠ” ì¤‘" : "ëŒ€ê¸° ì¤‘"}
                    className="w-4/5 h-4/5 object-contain"
                  />
                </div>
                
                {/* ë§í’ì„  ì˜ì—­ - ë…¹í™” ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ, ì—¬ìš° ìœ„ì— ê³ ì • ë°°ì¹˜ */}
                {isRecording && (
                  <div className="absolute w-[150px] h-[130px] sm:w-[170px] sm:h-[150px] md:w-[190px] md:h-[170px] lg:w-[210px] lg:h-[190px] xl:w-[240px] xl:h-[210px] z-10 pointer-events-none" style={{ top: 'calc(10% - 120px)', left: 'calc(40% + 40px)' }}>
                    <div className="h-full flex items-center justify-center">
                      {/* ëœë¤ ë§í’ì„  ì´ë¯¸ì§€ í‘œì‹œ */}
                      {currentBalloonImage && (
                        <img
                          src={currentBalloonImage}
                          alt={isSpeaking ? "ë§í•˜ëŠ” ì¤‘ ë§í’ì„ " : "ëŒ€ê¸° ì¤‘ ë§í’ì„ "}
                          className="max-w-full max-h-full object-contain transition-all duration-300"
                          onError={(e) => {
                            console.warn('ë§í’ì„  ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨:', currentBalloonImage)
                            // ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´
                            const target = e.target as HTMLImageElement
                            target.src = '/images/talkballoon/nottalk/1.png'
                          }}
                        />
                      )}
                    </div>
                  </div>
                )}
                
                {/* í”Œë ˆì´ìŠ¤í™€ë” (ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´) */}
                <div className="absolute inset-0 flex items-center justify-center text-gray-400" style={{ display: 'none' }}>
                  <Camera className="w-16 h-16 opacity-50" aria-hidden="true" />
                </div>
              </div>
            </div>
        </Card>
      </div>
      {/* ì™„ë£Œ ëª¨ë‹¬ */}
      <TrainingCompleteModal
        open={showCompleteModal}
        onClose={() => setShowCompleteModal(false)}
        title="í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        description="ìˆ˜ê³ í•˜ì…¨ì–´ìš”! í™•ì¸ì„ ëˆ„ë¥´ë©´ ê°ì • ê²°ê³¼ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."
        primaryActionLabel="í™•ì¸"
        onPrimaryAction={async () => {
          try {
            await fetch(`${process.env.NEXT_PUBLIC_BACKEND_URL}/api/cognitive/emotions?answerType=COGNITIVE_AUDIO`, {
              method: 'POST',
              credentials: 'include',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ emotion: (finalEmotion || 'NEUTRAL').toUpperCase() }),
            })
            // ê°ì • ë°ì´í„° ì „ì†¡ ì„±ê³µ ì‹œ ë¡œì»¬ìŠ¤í† ë¦¬ì§€ì— ì™„ë£Œ ìƒíƒœ ì €ì¥
            markRecallTrainingSessionAsCompleted('music')
          } catch (e) {
            console.error('ê°ì • ì „ì†¡ ì‹¤íŒ¨:', e)
          } finally {
            setShowCompleteModal(false)
            router.push('/main-elder/recall-training')
          }
        }}
      />
      
      {/* í”Œë¡œíŒ… ë²„íŠ¼ */}
      <FloatingButtons />
    </div>
  )
}
