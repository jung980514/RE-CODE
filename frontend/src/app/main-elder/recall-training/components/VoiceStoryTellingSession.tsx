"use client"

import { useState, useEffect, useRef, MutableRefObject } from "react"
import * as faceapi from '@vladmandic/face-api'
import { Button } from "@/components/ui/button"
import { synthesizeSpeech, playAudio, stopCurrentAudio } from "@/api/googleTTS/googleTTSService"
import { Card, CardContent } from "@/components/ui/card"
import { Progress } from "@/components/ui/progress"
import { WebcamView } from "@/components/common/WebcamView"
import {
  ArrowLeft,
  ArrowRight,
  Mic,
  MicOff,
  CheckCircle,
  Maximize,
  BookOpen,
  RotateCcw,
} from "lucide-react"
import { markRecallTrainingSessionAsCompleted } from "@/lib/auth"

interface VoiceSessionProps {
  onBack: () => void
}

// ê°ì • ë¶„ì„ í›…
interface EmotionRecord {
  timestamp: number;
  emotion: string;
  confidence: number;
}

function useEmotionDetection(videoRef: React.RefObject<HTMLVideoElement | null>, isRecording: boolean) {
  const [emotion, setEmotion] = useState<string>('ì¤‘ë¦½')
  const [confidence, setConfidence] = useState<number>(0)
  const requestRef = useRef<number | undefined>(undefined)
  const modelsLoaded = useRef<boolean>(false)
  const prevExpressions = useRef<faceapi.FaceExpressions | null>(null)
  const emotionHistory = useRef<EmotionRecord[]>([])
  const lastRecordTime = useRef<number>(0)

  useEffect(() => {
    const loadModels = async () => {
      try {
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model'),
          faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model')
        ])
        modelsLoaded.current = true
      } catch (error) {
        console.error('ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜:', error)
      }
    }
    loadModels()

    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [])

  const detectEmotion = async () => {
    if (!modelsLoaded.current || !videoRef.current) return

    try {
      const detections = await faceapi
        .detectSingleFace(videoRef.current, new faceapi.TinyFaceDetectorOptions())
        .withFaceExpressions()

      if (detections) {
        const expressions = detections.expressions
        
        // ê°ì • ì ìˆ˜ ì¡°ì •
        const emotionChange = prevExpressions.current ? 
          Math.abs(expressions.neutral - prevExpressions.current.neutral) : 0

        // ë§í•˜ê¸°/í‘œì • ë³€í™” ê°ì§€ ë¡œì§
        const isSpeaking = emotionChange > 0.15
        const isSmiling = expressions.happy > 0.3
        const isNeutralDominant = expressions.neutral > 0.5
        
        // ìŠ¬í”” ê°ì • ë³´ì •
        const sadScore = expressions.sad
        const mouthOpenScore = Math.max(expressions.surprised, expressions.sad)
        const isMouthOpen = mouthOpenScore > 0.3
        
        // ì‹¤ì œ ìŠ¬í”” í‘œì • íŒë‹¨
        const isActuallySad = sadScore > 0.4 && !isSpeaking && !isSmiling && !isMouthOpen
        
        const adjustedExpressions = {
          ...expressions,
          surprised: expressions.surprised * (isSpeaking ? 0.3 : 0.7),
          sad: expressions.sad * (isActuallySad ? 1.0 : 0.1),
          neutral: expressions.neutral * (isNeutralDominant ? 1.4 : 1.2),
          happy: expressions.happy * (isSmiling ? 1.5 : 1.2),
          angry: expressions.angry * (isSpeaking ? 0.7 : 0.9),
          fearful: expressions.fearful * (isSpeaking ? 0.7 : 0.9),
          disgusted: expressions.disgusted * 0.8
        }

        prevExpressions.current = expressions

        let maxExpression = 'neutral'
        let maxConfidence = adjustedExpressions.neutral
        const threshold = 0.3

        Object.entries(adjustedExpressions).forEach(([expression, value]) => {
          let currentThreshold = threshold
          if (expression === 'surprised') currentThreshold = threshold * 1.5
          if (expression === 'sad') currentThreshold = threshold * 1.4
          
          if (value > maxConfidence && value > currentThreshold) {
            if (isSpeaking && (expression === 'neutral' || expression === 'happy')) {
              maxConfidence = value * 1.2
            } else {
              maxConfidence = value
            }
            maxExpression = expression
          }
        })

        const emotionMap: { [key: string]: string } = {
          neutral: 'ì¤‘ë¦½',
          happy: 'í–‰ë³µ',
          sad: 'ìŠ¬í””',
          angry: 'í™”ë‚¨',
          fearful: 'ë‘ë ¤ì›€',
          disgusted: 'í˜ì˜¤',
          surprised: 'ë†€ëŒ'
        }

        const newEmotion = emotionMap[maxExpression] || 'ì¤‘ë¦½'
        if (maxConfidence > threshold) {
          setEmotion(newEmotion)
          setConfidence(maxConfidence)

          const now = Date.now()
          if (now - lastRecordTime.current >= 1000) {
            emotionHistory.current.push({
              timestamp: now,
              emotion: newEmotion,
              confidence: maxConfidence
            })
            lastRecordTime.current = now
          }
        }
      }
    } catch (error) {
      console.error('ê°ì • ë¶„ì„ ì˜¤ë¥˜:', error)
    }

    requestRef.current = requestAnimationFrame(detectEmotion)
  }

  const analyzeEmotionHistory = () => {
    if (emotionHistory.current.length === 0) return;

    const emotions = emotionHistory.current;
    const totalDuration = (emotions[emotions.length - 1].timestamp - emotions[0].timestamp) / 1000; // ì´ˆ ë‹¨ìœ„

    // ê° ê°ì •ë³„ ì§€ì† ì‹œê°„ ê³„ì‚°
    const emotionDurations: { [key: string]: number } = {};
    const emotionConfidences: { [key: string]: number[] } = {};

    emotions.forEach((record, index) => {
      const duration = index === emotions.length - 1
        ? 1 // ë§ˆì§€ë§‰ ê¸°ë¡ì€ 1ì´ˆë¡œ ê³„ì‚°
        : (emotions[index + 1].timestamp - record.timestamp) / 1000;

      emotionDurations[record.emotion] = (emotionDurations[record.emotion] || 0) + duration;
      emotionConfidences[record.emotion] = emotionConfidences[record.emotion] || [];
      emotionConfidences[record.emotion].push(record.confidence);
    });

    // ì¤‘ë¦½ì„ ì œì™¸í•œ ê°ì • ì¤‘ 17%ë¥¼ ë„˜ëŠ” ê°ì • ì°¾ê¸°
    const dominantEmotion = Object.entries(emotionDurations).find(([emotion, duration]) => {
      const percentage = (duration / totalDuration * 100);
      return emotion !== 'ì¤‘ë¦½' && percentage > 17;
    });

    // ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    console.log('=== ê°ì • ë¶„ì„ ê²°ê³¼ ===');
    console.log(`ì´ ë…¹í™” ì‹œê°„: ${totalDuration.toFixed(1)}ì´ˆ`);
    
    if (dominantEmotion) {
      // ì¤‘ë¦½ì„ ì œì™¸í•œ ê°ì •ì´ 17%ë¥¼ ë„˜ëŠ” ê²½ìš° í•´ë‹¹ ê°ì •ë§Œ ì¶œë ¥
      const [emotion, duration] = dominantEmotion;
      const percentage = (duration / totalDuration * 100).toFixed(1);
      const avgConfidence = (emotionConfidences[emotion].reduce((a, b) => a + b, 0) / emotionConfidences[emotion].length * 100).toFixed(1);
      console.log(`ì£¼ìš” ê°ì •: ${emotion} (${percentage}%, í‰ê·  ì‹ ë¢°ë„: ${avgConfidence}%)`);
    } else {
      // ì¤‘ë¦½ì„ ì œì™¸í•œ ê°ì •ì´ 17%ë¥¼ ë„˜ì§€ ì•ŠëŠ” ê²½ìš° ì¤‘ë¦½ë§Œ ì¶œë ¥
      const neutralDuration = emotionDurations['ì¤‘ë¦½'] || 0;
      const neutralPercentage = (neutralDuration / totalDuration * 100).toFixed(1);
      const neutralAvgConfidence = emotionConfidences['ì¤‘ë¦½'] 
        ? (emotionConfidences['ì¤‘ë¦½'].reduce((a, b) => a + b, 0) / emotionConfidences['ì¤‘ë¦½'].length * 100).toFixed(1)
        : '0.0';
      console.log(`ì£¼ìš” ê°ì •: ì¤‘ë¦½ (${neutralPercentage}%, í‰ê·  ì‹ ë¢°ë„: ${neutralAvgConfidence}%)`);
    }
    console.log('==================');

    // ê¸°ë¡ ì´ˆê¸°í™”
    emotionHistory.current = [];
    lastRecordTime.current = 0;
  };

  useEffect(() => {
    if (videoRef.current && isRecording && modelsLoaded.current) {
      detectEmotion()
      
      // ë…¹í™” ì‹œì‘ ì‹œ ê¸°ë¡ ì´ˆê¸°í™”
      if (emotionHistory.current.length === 0) {
        emotionHistory.current = [];
        lastRecordTime.current = Date.now();
      }
    } else {
      // ë…¹í™” ì¤‘ì§€ ì‹œì—ëŠ” ê°ì • ë¶„ì„ì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ (ìµœì¢… ì™„ë£Œ ì‹œì—ë§Œ ì‹¤í–‰)
      setEmotion('ì¤‘ë¦½')
      setConfidence(0)
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [videoRef.current, isRecording, modelsLoaded.current])

  return { emotion, confidence, analyzeEmotionHistory }
}

// ë¹„ë””ì˜¤ ë…¹í™” í›…
function useVideoRecording(videoStream: MediaStream | null) {
  const [isRecording, setIsRecording] = useState(false)
  const [audioLevel, setAudioLevel] = useState(0)
  const [recordedMedia, setRecordedMedia] = useState<string | null>(null)
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null)
  const [isAutoRecording, setIsAutoRecording] = useState(false)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const combinedStreamRef = useRef<MediaStream | null>(null)
  const chunksRef = useRef<Blob[]>([])
  const stopResolverRef = useRef<((blob: Blob) => void) | null>(null)

  const startRecording = async (isAuto = false) => {
    try {
      // ê¸°ì¡´ TTS ì •ì§€
      stopCurrentAudio()
      
      // ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (combinedStreamRef.current) {
        combinedStreamRef.current.getTracks().forEach((track) => track.stop())
      }
      
      const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const newVideoStream = await navigator.mediaDevices.getUserMedia({ video: true })
      
      const tracks = [...audioStream.getAudioTracks(), ...newVideoStream.getVideoTracks()]
      
      const combinedStream = new MediaStream(tracks)
      combinedStreamRef.current = combinedStream

      const mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType: "video/mp4",
      })
      mediaRecorderRef.current = mediaRecorder

      // ì˜¤ë””ì˜¤ ë ˆë²¨ ê°ì§€
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const microphone = audioContext.createMediaStreamSource(audioStream)
      microphone.connect(analyser)
      audioContextRef.current = audioContext

      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      const updateAudioLevel = () => {
        if (isRecording) {
          analyser.getByteFrequencyData(dataArray)
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length
          setAudioLevel(average)
          requestAnimationFrame(updateAudioLevel)
        }
      }

      mediaRecorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          chunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        try {
          const finalBlob = new Blob(chunksRef.current, { type: 'video/mp4' })
          setRecordedBlob(finalBlob)
          const mediaUrl = URL.createObjectURL(finalBlob)
          setRecordedMedia(mediaUrl)
          if (stopResolverRef.current) {
            stopResolverRef.current(finalBlob)
            stopResolverRef.current = null
          }
        } catch (e) {
          console.error('ë…¹í™” ì¢…ë£Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', e)
        } finally {
          chunksRef.current = []
        }
      }

      mediaRecorder.start()
      setIsRecording(true)
      setIsAutoRecording(isAuto)
      updateAudioLevel()
    } catch (error) {
      console.error("ë…¹í™” ì˜¤ë¥˜:", error)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      setIsAutoRecording(false)
      setAudioLevel(0)

      // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (combinedStreamRef.current) {
        combinedStreamRef.current.getTracks().forEach((track) => track.stop())
      }

      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }

  const resetRecording = () => {
    // ë…¹í™” ì¤‘ì´ë©´ ì¤‘ì§€
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
    }
    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (combinedStreamRef.current) {
      combinedStreamRef.current.getTracks().forEach((track) => track.stop())
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
    }
    // ê¸°ì¡´ URL ì •ë¦¬
    if (recordedMedia) {
      URL.revokeObjectURL(recordedMedia)
    }
    // ìƒíƒœ ì´ˆê¸°í™”
    setIsRecording(false)
    setIsAutoRecording(false)
    setAudioLevel(0)
    setRecordedMedia(null)
    setRecordedBlob(null)
    // ref ì •ë¦¬
    mediaRecorderRef.current = null
    audioContextRef.current = null
    combinedStreamRef.current = null
    chunksRef.current = []
  }

  const stopAndGetBlob = async (): Promise<Blob> => {
    if (recordedBlob && !isRecording) {
      return recordedBlob
    }
    if (mediaRecorderRef.current && isRecording) {
      return new Promise<Blob>((resolve) => {
        stopResolverRef.current = resolve
        mediaRecorderRef.current?.stop()
        setIsRecording(false)
        setIsAutoRecording(false)
        setAudioLevel(0)
        // ìŠ¤íŠ¸ë¦¼ ë° ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
        if (combinedStreamRef.current) {
          combinedStreamRef.current.getTracks().forEach((track) => track.stop())
        }
        if (audioContextRef.current) {
          audioContextRef.current.close()
        }
      })
    }
    throw new Error('ë…¹í™”ëœ ë¸”ëì´ ì—†ìŠµë‹ˆë‹¤.')
  }

  return {
    isRecording,
    audioLevel,
    recordedMedia,
    recordedBlob,
    isAutoRecording,
    startRecording,
    stopRecording,
    resetRecording,
    stopAndGetBlob,
  }
}

export function VoiceStoryTellingSession({ onBack }: VoiceSessionProps) {
  const [currentTopic, setCurrentTopic] = useState(0)
  const [isAITalking, setIsAITalking] = useState(true)
  const [showCompletionModal, setShowCompletionModal] = useState(false)
  const [webcamStream, setWebcamStream] = useState<MediaStream | null>(null)
  const [hasStartedRecording, setHasStartedRecording] = useState(false)
  const [userId, setUserId] = useState<number | null>(null)
  const [topics, setTopics] = useState<Array<{ title: string; question: string }>>([])
  const [topicIds, setTopicIds] = useState<number[]>([])
  const videoRef = useRef<HTMLVideoElement>(null)
  const { isRecording, audioLevel, recordedMedia, recordedBlob, isAutoRecording, startRecording, stopRecording, resetRecording, stopAndGetBlob } = useVideoRecording(webcamStream)
  const { emotion, confidence, analyzeEmotionHistory } = useEmotionDetection(videoRef, isRecording)
  const topicIcons = ["ğŸŒŸ", "ğŸŒŸ", "ğŸŒŸ", "ğŸŒŸ", "ğŸŒŸ"]

  // ì‚¬ìš©ì ë° ì§ˆë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°
  useEffect(() => {
    const fetchUser = async () => {
      try {
        const stored = typeof window !== 'undefined' ? localStorage.getItem('userId') : null
        if (stored) {
          const parsed = Number(stored)
          if (!Number.isNaN(parsed)) setUserId(parsed)
        }
        const res = await fetch('https://recode-my-life.site/api/user', {
          method: 'GET',
          credentials: 'include',
          headers: { 'Accept': 'application/json' },
        })
        if (res.ok) {
          const json = await res.json()
          const idCandidate = json?.data?.id
          if (typeof idCandidate === 'number') {
            setUserId(idCandidate)
            try { localStorage.setItem('userId', String(idCandidate)) } catch {}
          }
        }
      } catch (e) {
        console.error('ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜:', e)
      }
    }

    const fetchQuestions = async () => {
      try {
        const response = await fetch('https://recode-my-life.site/api/personal/questions', {
          method: 'GET',
          credentials: 'include',
          headers: { 'Accept': 'application/json' },
        })
        if (!response.ok) throw new Error(`ì§ˆë¬¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: ${response.status}`)
        const result = await response.json()
        // API ì‘ë‹µ íƒ€ì… ì •ì˜ë¡œ any ì œê±°
        interface PersonalQuestionRaw {
          id?: number
          questionId?: number
          personalQuestionId?: number
          content?: string
          questionContent?: string
          [key: string]: unknown
        }
        const raw: PersonalQuestionRaw[] = Array.isArray(result?.data)
          ? (result.data as PersonalQuestionRaw[])
          : []
        // ì„œë²„ ìŠ¤í‚¤ë§ˆ(id | questionId | personalQuestionId) ëŒ€ì‘í•˜ì—¬ topic/ids ë™ê¸°í™” êµ¬ì„±
        interface Pair { id: number | undefined; topic: { title: string; question: string } }
        const mappedPairs: Pair[] = raw.map((item): Pair => {
          const idCandidate: number | undefined =
            typeof item?.id === 'number' ? item.id
            : typeof item?.questionId === 'number' ? item.questionId
            : typeof item?.personalQuestionId === 'number' ? item.personalQuestionId
            : undefined

          const content: string = typeof item?.content === 'string'
            ? item.content
            : (typeof item?.questionContent === 'string' ? item.questionContent : '')

          return {
            id: idCandidate,
            topic: {
              title: 'ê°œì¸í™” ì§ˆë¬¸',
              question: `${content}\nì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ ë‹µë³€í•˜ê¸°ë¥¼ ëˆŒëŸ¬ ì‹œì‘í•´ì£¼ì„¸ìš”`,
            }
          }
        })

        const validPairs: Pair[] = mappedPairs.filter((p: Pair) => typeof p.id === 'number')
        if (validPairs.length !== mappedPairs.length) {
          console.warn(`ê°œì¸í™” ì§ˆë¬¸ IDê°€ ì—†ëŠ” í•­ëª©ì´ ${mappedPairs.length - validPairs.length}ê°œ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ì¼ë¶€:`, raw)
        }
        setTopics(validPairs.map((p: Pair) => p.topic))
        setTopicIds(validPairs.map((p: Pair) => p.id as number))
        setCurrentTopic(0)
        setHasStartedRecording(false)
      } catch (e) {
        console.error(e)
      }
    }

    fetchUser()
    fetchQuestions()
  }, [])

  useEffect(() => {
    // ì£¼ì œê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ TTS ì¬ìƒ
    const playTopicTTS = async () => {
      try {
        setIsAITalking(true)
        if (!topics[currentTopic]) return
        const audioContent = await synthesizeSpeech(topics[currentTopic].question)
        await playAudio(audioContent)
      } catch (error) {
        console.error('TTS ì—ëŸ¬:', error)
      } finally {
        setIsAITalking(false)
      }
    }
    
    playTopicTTS()

    // ì»´í¬ë„ŒíŠ¸ê°€ ì–¸ë§ˆìš´íŠ¸ë  ë•Œ TTS ì •ì§€
    return () => {
      stopCurrentAudio()
    }
  }, [currentTopic, topics])

  // ì£¼ì œê°€ ë°”ë€Œë©´ ë‹¤ìŒ ë²„íŠ¼ì„ ë¹„í™œì„±í™” ìƒíƒœë¡œ ì´ˆê¸°í™”
  useEffect(() => {
    setHasStartedRecording(false)
  }, [currentTopic])

  const handleNext = async () => {
    if (topics.length === 0) return
    try {
      const questionId = topicIds[currentTopic]
      console.log('ì—…ë¡œë“œì— ì‚¬ìš©í•  questionId:', questionId)
      // ì—…ë¡œë“œ ì§ì „ì— userId ì—†ìœ¼ë©´ ì¡°íšŒ ì‹œë„
      let ensuredUserId = userId
      if (ensuredUserId == null) {
        try {
          const res = await fetch('https://recode-my-life.site/api/user', {
            method: 'GET',
            credentials: 'include',
            headers: { 'Accept': 'application/json' },
          })
          if (res.ok) {
            const json = await res.json()
            const idCandidate = json?.data?.id
            if (typeof idCandidate === 'number') {
              ensuredUserId = idCandidate
              setUserId(idCandidate)
              try { localStorage.setItem('userId', String(idCandidate)) } catch {}
            }
          }
        } catch {}
      }

      let blobToUpload = recordedBlob
      if (isRecording) {
        blobToUpload = await stopAndGetBlob()
      }
      if (!blobToUpload) {
        console.warn('ì—…ë¡œë“œí•  ë…¹í™” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
      } else {
        const formData = new FormData()
        formData.append('questionId', String(questionId))
        if (ensuredUserId != null) {
          formData.append('userId', String(ensuredUserId))
        }
        formData.append('mediaType', 'video')
        const fileName = `answer_${questionId}_${new Date().toISOString().replace(/[:.]/g, '-')}.mp4`
        const file = new File([blobToUpload], fileName, { type: 'video/mp4' })
        formData.append('videoFile', file)

        const uploadResponse = await fetch('https://recode-my-life.site/api/personal/answers', {
          method: 'POST',
          credentials: 'include',
          body: formData,
        })
        if (!uploadResponse.ok) {
          console.log(uploadResponse)
          console.error('ë‹µë³€ ì—…ë¡œë“œ ì‹¤íŒ¨:', uploadResponse.status)
        }
      }

    if (currentTopic < topics.length - 1) {
      setCurrentTopic(currentTopic + 1)
      if (isRecording) {
        stopRecording()
      }
      resetRecording()
    } else {
      console.log('=== ì´ì•¼ê¸° ë‚˜ëˆ„ê¸° í›ˆë ¨ ìµœì¢… ê°ì • ë¶„ì„ ê²°ê³¼ ===')
      analyzeEmotionHistory()
      markRecallTrainingSessionAsCompleted('story')
      setShowCompletionModal(true)
      }
    } catch (err) {
      console.error('ë‹¤ìŒìœ¼ë¡œ ì§„í–‰ ì¤‘ ì˜¤ë¥˜:', err)
    }
  }

  // ë‹µë³€í•˜ê¸°(ë…¹ìŒ ì‹œì‘) ë²„íŠ¼ í´ë¦­ ì‹œ: ë…¹ìŒ ì‹œì‘ê³¼ ë™ì‹œì— ë‹¤ìŒ ë²„íŠ¼ í™œì„±í™”
  const handleStartRecording = () => {
    startRecording(false)
    setHasStartedRecording(true)
  }

  const handleBackToMain = () => {
    stopCurrentAudio()
    onBack()
  }

  const replayQuestion = async () => {
    try {
      // ê¸°ì¡´ TTS ì •ì§€
      stopCurrentAudio()
      setIsAITalking(true)
      if (isRecording) {
        stopRecording()
      }
      const audioContent = await synthesizeSpeech(topics[currentTopic].question)
      await playAudio(audioContent)
    } catch (error) {
      console.error('TTS ì—ëŸ¬:', error)
    } finally {
      setIsAITalking(false)
    }
  }

  // ì™„ë£Œ ëª¨ë‹¬
  if (showCompletionModal) {
    return (
      <div className="min-h-screen bg-gradient-to-br from-orange-50 to-red-100 flex items-center justify-center p-6">
        <Card className="max-w-md w-full bg-white/95 backdrop-blur border-0 shadow-2xl">
          <CardContent className="p-8 text-center">
            <div className="w-20 h-20 bg-green-100 rounded-full flex items-center justify-center mx-auto mb-6">
              <CheckCircle className="w-10 h-10 text-green-600" />
            </div>
            <h2 className="text-4xl font-bold text-gray-800 mb-6">ì´ì•¼ê¸° ë‚˜ëˆ„ê¸° ì™„ë£Œ!</h2>
            <p className="text-2xl text-gray-600 mb-10 font-medium">
              ëª¨ë“  ì£¼ì œë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤.<br />
              ë‹¤ë¥¸ í›ˆë ¨ í”„ë¡œê·¸ë¨ë„ ì§„í–‰í•´ë³´ì„¸ìš”.
            </p>
            <div className="flex gap-6">
              <Button
                variant="outline"
                onClick={handleBackToMain}
                className="flex-1 h-16 text-xl font-bold px-8"
              >
                ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸°
              </Button>
            </div>
          </CardContent>
        </Card>
      </div>
    )
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-orange-50 to-red-100 p-4">
      <div className="max-w-7xl mx-auto">
        {/* í—¤ë” */}
        <div className="flex items-center justify-between mb-6">
          <Button variant="ghost" onClick={onBack} className="flex items-center gap-3 text-2xl font-bold hover:bg-white/50 bg-white/80 backdrop-blur px-6 py-4">
            <ArrowLeft className="w-7 h-7" />
            ëŒì•„ê°€ê¸°
          </Button>
          <div className="text-center">
            <h1 className="text-5xl font-bold text-gray-800 mb-2" style={{ fontFamily: "Paperlogy, sans-serif" }}>
              ì´ì•¼ê¸° ë‚˜ëˆ„ê¸° í›ˆë ¨
            </h1>
            <p className="text-2xl text-gray-600 font-medium">ê°œì¸í™”ëœ ì‹¬ì¸µ ì§ˆë¬¸ìœ¼ë¡œ ì†Œì¤‘í•œ ì¶”ì–µì„ ë˜ì‚´ë ¤ë³´ì„¸ìš”</p>
          </div>
          <div className="text-right">
          </div>
        </div>

        {/* ë©”ì¸ ì½˜í…ì¸  */}
        <div className="grid lg:grid-cols-3 gap-6">
          {/* ì™¼ìª½: ì£¼ì œì™€ ì§ˆë¬¸ ì˜ì—­ */}
          <div className="lg:col-span-2">
            <Card className="shadow-2xl border-0 bg-white/95 backdrop-blur overflow-hidden h-full">
              <CardContent className="p-8">
                {/* ì£¼ì œ ì œëª© */}
                <div className="text-center mb-8">
                  <div className="inline-flex items-center gap-4 bg-orange-100 text-orange-700 px-6 py-3 rounded-full mb-6">
                    <BookOpen className="w-7 h-7" />
                    <span className="font-bold text-xl">
                      ì£¼ì œ {currentTopic + 1}/{topics.length}
                    </span>
                  </div>
                  <div className="text-8xl mb-6">{topicIcons[(topics.length > 0 ? currentTopic : 0) % topicIcons.length]}</div>
                  <h2 className="text-4xl font-bold text-gray-800 mb-6">{topics[currentTopic]?.title ?? 'ê°œì¸í™” ì§ˆë¬¸'}</h2>
                </div>

                {/* ì§ˆë¬¸ ë‚´ìš© */}
                <div className="bg-gradient-to-r from-orange-50 to-red-50 p-10 rounded-2xl mb-10">
                  <div className="flex items-start gap-6">
                    <div className="w-16 h-16 bg-orange-500 rounded-full flex items-center justify-center flex-shrink-0">
                      <BookOpen className="w-8 h-8 text-white" />
                    </div>
                    <div className="flex-1">
                      <p className="text-lg text-orange-600 font-bold mb-3">ì§ˆë¬¸ì„ ì½ì–´ì£¼ì„¸ìš”</p>
                      <p className="text-2xl leading-relaxed text-gray-800 whitespace-pre-line font-medium">
                        {topics[currentTopic]?.question ?? 'ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...'}
                      </p>
                    </div>
                  </div>
                </div>
                {/* ë…¹ìŒ ìƒíƒœ */}
                {isRecording && (
                  <div className="mb-8">
                    <div className="bg-red-50 border border-red-200 rounded-lg p-6">
                      <div className="flex items-center justify-between">
                        <div className="flex items-center gap-4">
                          <div className="w-6 h-6 bg-red-500 rounded-full animate-pulse"></div>
                          <div>
                            <p className="font-bold text-red-800 text-xl">ë…¹ìŒ ì¤‘...</p>
                            <p className="text-lg text-red-600">ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”</p>
                          </div>
                        </div>
                        <div className="flex items-center gap-3">
                          <div className="w-4 h-4 bg-red-500 rounded-full animate-pulse"></div>
                          <div className="w-4 h-4 bg-red-500 rounded-full animate-pulse" style={{ animationDelay: '0.2s' }}></div>
                          <div className="w-4 h-4 bg-red-500 rounded-full animate-pulse" style={{ animationDelay: '0.4s' }}></div>
                        </div>
                      </div>
                      
                      {/* ì˜¤ë””ì˜¤ ë ˆë²¨ í‘œì‹œ */}
                      <div className="mt-4">
                        <div className="flex items-center gap-1 h-8">
                          {Array.from({ length: 20 }, (_, i) => (
                            <div
                              key={i}
                              className="flex-1 bg-red-200 rounded-sm transition-all duration-100"
                              style={{
                                height: `${Math.max(10, (audioLevel / 255) * 100 * (i + 1) / 20)}%`,
                                backgroundColor: audioLevel > 50 ? '#ef4444' : '#fecaca'
                              }}
                            />
                          ))}
                        </div>
                      </div>
                    </div>
                  </div>
                )}

                {/* ë…¹í™”ëœ ë¯¸ë””ì–´ ì¬ìƒ */}
                {recordedMedia && !isRecording && (
                  <div className="mb-8">
                    <div className="bg-green-50 border border-green-200 rounded-lg p-6">
                      <div className="flex items-center justify-between">
                        <div className="flex items-center gap-4">
                          <CheckCircle className="w-8 h-8 text-green-600" />
                          <div>
                            <p className="font-bold text-green-800 text-xl">ë‹µë³€ì´ ë…¹í™”ë˜ì—ˆìŠµë‹ˆë‹¤</p>
                            <p className="text-lg text-green-600">ì•„ë˜ì—ì„œ ë‹¤ì‹œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤</p>
                          </div>
                        </div>
                        <video controls src={recordedMedia} className="w-100 h-31 rounded-lg" />
                      </div>
                    </div>
                  </div>
                )}

                {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ë“¤ */}
                <div className="flex items-center justify-center gap-6">
                  <Button
                    onClick={replayQuestion}
                    variant="outline"
                    className="h-16 px-8 border-2 border-orange-400 text-orange-700 hover:bg-orange-50 bg-transparent text-xl font-bold"
                  >
                    <RotateCcw className="w-6 h-6 mr-3" />
                    ë‹¤ì‹œì¬ìƒ
                  </Button>

                  <Button
                    onClick={isRecording ? stopRecording : handleStartRecording}
                    className={`h-16 px-12 text-xl font-bold ${
                      isRecording
                        ? "bg-red-600 hover:bg-red-700 text-white"
                        : "bg-green-600 hover:bg-green-700 text-white"
                    }`}
                  >
                    {isRecording ? (
                      <>
                        <MicOff className="w-6 h-6 mr-3" />
                        ë…¹ìŒ ì¤‘ì§€
                      </>
                    ) : (
                      <>
                        <Mic className="w-6 h-6 mr-3" />
                        ë‹µë³€í•˜ê¸°
                      </>
                    )}
                  </Button>

                  <Button
                    onClick={handleNext}
                    disabled={!hasStartedRecording}
                    className="h-16 px-12 bg-orange-600 hover:bg-orange-700 text-white text-xl font-bold"
                  >
                    {currentTopic === topics.length - 1 ? 'ì™„ë£Œí•˜ê¸°' : 'ë‹¤ìŒ ì£¼ì œ'}
                    <ArrowRight className="w-6 h-6 ml-3" />
                  </Button>
                </div>
              </CardContent>
            </Card>
          </div>

          {/* ì˜¤ë¥¸ìª½: ì‚¬ìš©ì ìº  í™”ë©´ */}
          <div className="lg:col-span-1">
            <Card className="shadow-2xl border-0 bg-white/95 backdrop-blur overflow-hidden h-full">
              <CardContent className="p-6">
                <div className="space-y-4">
                  <WebcamView 
                    isRecording={isRecording}
                    onStreamReady={setWebcamStream}
                    videoRef={videoRef}
                  />
                  
                  {/* ê°ì • ë¶„ì„ ê²°ê³¼ */}
                  <div className="bg-white/80 backdrop-blur rounded-lg p-6">
                    <h3 className="text-2xl font-bold mb-4">ê°ì • ë¶„ì„</h3>
                    <div className="space-y-4">
                      <div className="flex justify-between items-center">
                        <span className="text-lg font-medium">í˜„ì¬ ê°ì •:</span>
                        <span className="font-bold text-orange-600 text-xl">{emotion}</span>
                      </div>
                      <div className="flex justify-between items-center">
                        <span className="text-lg font-medium">ì‹ ë¢°ë„:</span>
                        <span className="font-bold text-orange-600 text-xl">{Math.round(confidence * 100)}%</span>
                      </div>
                      <div className="w-full bg-gray-200 rounded-full h-3">
                        <div
                          className="bg-orange-600 h-3 rounded-full transition-all duration-300"
                          style={{ width: `${confidence * 100}%` }}
                        />
                      </div>
                    </div>
                  </div>
                </div>
              </CardContent>
            </Card>
          </div>
        </div>
      </div>
    </div>
  )
} 