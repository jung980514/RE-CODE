"use client"

import { useState, useEffect, useRef, MutableRefObject } from "react"
import * as faceapi from '@vladmandic/face-api'
import { Button } from "@/components/ui/button"
import { synthesizeSpeech, playAudio, stopCurrentAudio } from "@/api/googleTTS/googleTTSService"
import { Card, CardContent } from "@/components/ui/card"
import { Progress } from "@/components/ui/progress"
import { WebcamView } from "@/components/common/WebcamView"
import {
  ArrowLeft,
  ArrowRight,
  Mic,
  MicOff,
  CheckCircle,
  Maximize,
  BookOpen,
  RotateCcw,
} from "lucide-react"
import { markRecallTrainingSessionAsCompleted } from "@/lib/auth"

interface VoiceSessionProps {
  onBack: () => void
}

// ê°ì • ë¶„ì„ í›…
interface EmotionRecord {
  timestamp: number;
  emotion: string;
  confidence: number;
}

function useEmotionDetection(videoRef: React.RefObject<HTMLVideoElement | null>, isRecording: boolean) {
  const [emotion, setEmotion] = useState<string>('ì¤‘ë¦½')
  const [confidence, setConfidence] = useState<number>(0)
  const requestRef = useRef<number | undefined>(undefined)
  const modelsLoaded = useRef<boolean>(false)
  const prevExpressions = useRef<faceapi.FaceExpressions | null>(null)
  const emotionHistory = useRef<EmotionRecord[]>([])
  const lastRecordTime = useRef<number>(0)

  useEffect(() => {
    const loadModels = async () => {
      try {
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model'),
          faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model')
        ])
        modelsLoaded.current = true
      } catch (error) {
        console.error('ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜:', error)
      }
    }
    loadModels()

    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [])

  const detectEmotion = async () => {
    if (!modelsLoaded.current || !videoRef.current) return

    try {
      const detections = await faceapi
        .detectSingleFace(videoRef.current, new faceapi.TinyFaceDetectorOptions())
        .withFaceExpressions()

      if (detections) {
        const expressions = detections.expressions
        
        // ê°ì • ì ìˆ˜ ì¡°ì •
        const emotionChange = prevExpressions.current ? 
          Math.abs(expressions.neutral - prevExpressions.current.neutral) : 0

        // ë§í•˜ê¸°/í‘œì • ë³€í™” ê°ì§€ ë¡œì§
        const isSpeaking = emotionChange > 0.15
        const isSmiling = expressions.happy > 0.3
        const isNeutralDominant = expressions.neutral > 0.5
        
        // ìŠ¬í”” ê°ì • ë³´ì •
        const sadScore = expressions.sad
        const mouthOpenScore = Math.max(expressions.surprised, expressions.sad)
        const isMouthOpen = mouthOpenScore > 0.3
        
        // ì‹¤ì œ ìŠ¬í”” í‘œì • íŒë‹¨
        const isActuallySad = sadScore > 0.4 && !isSpeaking && !isSmiling && !isMouthOpen
        
        const adjustedExpressions = {
          ...expressions,
          surprised: expressions.surprised * (isSpeaking ? 0.3 : 0.7),
          sad: expressions.sad * (isActuallySad ? 1.0 : 0.1),
          neutral: expressions.neutral * (isNeutralDominant ? 1.4 : 1.2),
          happy: expressions.happy * (isSmiling ? 1.5 : 1.2),
          angry: expressions.angry * (isSpeaking ? 0.7 : 0.9),
          fearful: expressions.fearful * (isSpeaking ? 0.7 : 0.9),
          disgusted: expressions.disgusted * 0.8
        }

        prevExpressions.current = expressions

        let maxExpression = 'neutral'
        let maxConfidence = adjustedExpressions.neutral
        const threshold = 0.3

        Object.entries(adjustedExpressions).forEach(([expression, value]) => {
          let currentThreshold = threshold
          if (expression === 'surprised') currentThreshold = threshold * 1.5
          if (expression === 'sad') currentThreshold = threshold * 1.4
          
          if (value > maxConfidence && value > currentThreshold) {
            if (isSpeaking && (expression === 'neutral' || expression === 'happy')) {
              maxConfidence = value * 1.2
            } else {
              maxConfidence = value
            }
            maxExpression = expression
          }
        })

        const emotionMap: { [key: string]: string } = {
          neutral: 'ì¤‘ë¦½',
          happy: 'í–‰ë³µ',
          sad: 'ìŠ¬í””',
          angry: 'í™”ë‚¨',
          fearful: 'ë‘ë ¤ì›€',
          disgusted: 'í˜ì˜¤',
          surprised: 'ë†€ëŒ'
        }

        const newEmotion = emotionMap[maxExpression] || 'ì¤‘ë¦½'
        if (maxConfidence > threshold) {
          setEmotion(newEmotion)
          setConfidence(maxConfidence)

          const now = Date.now()
          if (now - lastRecordTime.current >= 1000) {
            emotionHistory.current.push({
              timestamp: now,
              emotion: newEmotion,
              confidence: maxConfidence
            })
            lastRecordTime.current = now
          }
        }
      }
    } catch (error) {
      console.error('ê°ì • ë¶„ì„ ì˜¤ë¥˜:', error)
    }

    requestRef.current = requestAnimationFrame(detectEmotion)
  }

  const analyzeEmotionHistory = () => {
    if (emotionHistory.current.length === 0) return;

    const emotions = emotionHistory.current;
    const totalDuration = (emotions[emotions.length - 1].timestamp - emotions[0].timestamp) / 1000;

    const emotionDurations: { [key: string]: number } = {};
    const emotionConfidences: { [key: string]: number[] } = {};

    emotions.forEach((record, index) => {
      const duration = index === emotions.length - 1
        ? 1
        : (emotions[index + 1].timestamp - record.timestamp) / 1000;

      emotionDurations[record.emotion] = (emotionDurations[record.emotion] || 0) + duration;
      emotionConfidences[record.emotion] = emotionConfidences[record.emotion] || [];
      emotionConfidences[record.emotion].push(record.confidence);
    });

    console.log('=== ê°ì • ë¶„ì„ ê²°ê³¼ ===');
    console.log(`ì´ ë…¹í™” ì‹œê°„: ${totalDuration.toFixed(1)}ì´ˆ`);
    Object.entries(emotionDurations).forEach(([emotion, duration]) => {
      const percentage = (duration / totalDuration * 100).toFixed(1);
      const avgConfidence = (emotionConfidences[emotion].reduce((a, b) => a + b, 0) / emotionConfidences[emotion].length * 100).toFixed(1);
      console.log(`${emotion}: ${percentage}% (${duration.toFixed(1)}ì´ˆ, í‰ê·  ì‹ ë¢°ë„: ${avgConfidence}%)`);
    });
    console.log('==================');

    emotionHistory.current = [];
    lastRecordTime.current = 0;
  };

  useEffect(() => {
    if (videoRef.current && isRecording && modelsLoaded.current) {
      detectEmotion()
      
      if (emotionHistory.current.length === 0) {
        emotionHistory.current = [];
        lastRecordTime.current = Date.now();
      }
    } else {
      if (!isRecording && emotionHistory.current.length > 0) {
        analyzeEmotionHistory();
      }

      setEmotion('ì¤‘ë¦½')
      setConfidence(0)
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current)
      }
    }
  }, [videoRef.current, isRecording, modelsLoaded.current])

  return { emotion, confidence }
}

// ë¹„ë””ì˜¤ ë…¹í™” í›…
function useVideoRecording(videoStream: MediaStream | null) {
  const [isRecording, setIsRecording] = useState(false)
  const [audioLevel, setAudioLevel] = useState(0)
  const [recordedMedia, setRecordedMedia] = useState<string | null>(null)
  const [isAutoRecording, setIsAutoRecording] = useState(false)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const combinedStreamRef = useRef<MediaStream | null>(null)

  const startRecording = async (isAuto = false) => {
    try {
      // ê¸°ì¡´ TTS ì •ì§€
      stopCurrentAudio()
      
      // ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (combinedStreamRef.current) {
        combinedStreamRef.current.getTracks().forEach((track) => track.stop())
      }
      
      const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const newVideoStream = await navigator.mediaDevices.getUserMedia({ video: true })
      
      const tracks = [...audioStream.getAudioTracks(), ...newVideoStream.getVideoTracks()]
      
      const combinedStream = new MediaStream(tracks)
      combinedStreamRef.current = combinedStream

      const mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType: "video/mp4",
      })
      mediaRecorderRef.current = mediaRecorder

      // ì˜¤ë””ì˜¤ ë ˆë²¨ ê°ì§€
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const microphone = audioContext.createMediaStreamSource(audioStream)
      microphone.connect(analyser)
      audioContextRef.current = audioContext

      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      const updateAudioLevel = () => {
        if (isRecording) {
          analyser.getByteFrequencyData(dataArray)
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length
          setAudioLevel(average)
          requestAnimationFrame(updateAudioLevel)
        }
      }

      mediaRecorder.start()
      setIsRecording(true)
      setIsAutoRecording(isAuto)
      updateAudioLevel()

      mediaRecorder.ondataavailable = (event) => {
        const mediaBlob = new Blob([event.data], { type: "video/mp4" })
        const mediaUrl = URL.createObjectURL(mediaBlob)
        setRecordedMedia(mediaUrl)
      }
    } catch (error) {
      console.error("ë…¹í™” ì˜¤ë¥˜:", error)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      setIsAutoRecording(false)
      setAudioLevel(0)

      // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (combinedStreamRef.current) {
        combinedStreamRef.current.getTracks().forEach((track) => track.stop())
      }

      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }

  return {
    isRecording,
    audioLevel,
    recordedMedia,
    isAutoRecording,
    startRecording,
    stopRecording,
  }
}

export function VoiceStoryTellingSession({ onBack }: VoiceSessionProps) {
  const [currentTopic, setCurrentTopic] = useState(0)
  const [isAITalking, setIsAITalking] = useState(true)
  const [showCompletionModal, setShowCompletionModal] = useState(false)
  const [webcamStream, setWebcamStream] = useState<MediaStream | null>(null)
  const videoRef = useRef<HTMLVideoElement>(null)
  const { isRecording, audioLevel, recordedMedia, isAutoRecording, startRecording, stopRecording } = useVideoRecording(webcamStream)
  const { emotion, confidence } = useEmotionDetection(videoRef, isRecording)

  const topics = [
    {
      title: "ê°œì¸í™” ì§ˆë¬¸",
      question: "ì¸ìƒì—ì„œ ê°€ì¥ í–‰ë³µí–ˆë˜ ìˆœê°„ì€ ì–¸ì œì˜€ë‚˜ìš”?\nê·¸ë•Œì˜ ê¸°ë¶„ê³¼ ì£¼ë³€ ì‚¬ëŒë“¤ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ ë‹µë³€í•˜ê¸°ë¥¼ ëˆŒëŸ¬ ì‹œì‘í•´ì£¼ì„¸ìš”",
      icon: "ğŸŒŸ",
    },
  ]

  useEffect(() => {
    replayQuestion()

    // ì»´í¬ë„ŒíŠ¸ê°€ ì–¸ë§ˆìš´íŠ¸ë  ë•Œ TTS ì •ì§€
    return () => {
      stopCurrentAudio()
    }
  }, [currentTopic])

  const handleNext = () => {
    if (currentTopic < topics.length - 1) {
      setCurrentTopic(currentTopic + 1)
      setIsAITalking(true)
      if (isRecording) {
        stopRecording()
      }
    } else {
      // ë§ˆì§€ë§‰ ì§ˆë¬¸ ì™„ë£Œ ì‹œ
      markRecallTrainingSessionAsCompleted('story')
      setShowCompletionModal(true)
    }
  }

  const handleBackToMain = () => {
    onBack()
  }

  const replayQuestion = async () => {
    try {
      // ê¸°ì¡´ TTS ì •ì§€
      stopCurrentAudio()
      setIsAITalking(true)
      if (isRecording) {
        stopRecording()
      }
      const audioContent = await synthesizeSpeech(topics[currentTopic].question)
      await playAudio(audioContent)
    } catch (error) {
      console.error('TTS ì—ëŸ¬:', error)
    } finally {
      setIsAITalking(false)
    }
  }

  // ì™„ë£Œ ëª¨ë‹¬
  if (showCompletionModal) {
    return (
      <div className="min-h-screen bg-gradient-to-br from-orange-50 to-red-100 flex items-center justify-center p-6">
        <Card className="max-w-md w-full bg-white/95 backdrop-blur border-0 shadow-2xl">
          <CardContent className="p-8 text-center">
            <div className="w-20 h-20 bg-green-100 rounded-full flex items-center justify-center mx-auto mb-6">
              <CheckCircle className="w-10 h-10 text-green-600" />
            </div>
            <h2 className="text-2xl font-bold text-gray-800 mb-4">ì´ì•¼ê¸° ë‚˜ëˆ„ê¸° ì™„ë£Œ!</h2>
            <p className="text-gray-600 mb-8">
              ëª¨ë“  ì£¼ì œë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤.<br />
              ë‹¤ë¥¸ í›ˆë ¨ í”„ë¡œê·¸ë¨ë„ ì§„í–‰í•´ë³´ì„¸ìš”.
            </p>
            <div className="flex gap-4">
              <Button
                variant="outline"
                onClick={handleBackToMain}
                className="flex-1"
              >
                ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸°
              </Button>
            </div>
          </CardContent>
        </Card>
      </div>
    )
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-orange-50 to-red-100 p-4">
      <div className="max-w-7xl mx-auto">
        {/* í—¤ë” */}
        <div className="flex items-center justify-between mb-6">
          <Button variant="ghost" onClick={onBack} className="flex items-center gap-2 text-lg">
            <ArrowLeft className="w-5 h-5" />
            ëŒì•„ê°€ê¸°
          </Button>
          <div className="flex items-center gap-3">
            <h1 className="text-2xl font-bold text-gray-800">ê°œì¸í™” í›ˆë ¨</h1>
          </div>
          <div className="text-right">
          </div>
        </div>

        {/* ë©”ì¸ ì½˜í…ì¸  */}
        <div className="grid lg:grid-cols-3 gap-6">
          {/* ì™¼ìª½: ì£¼ì œì™€ ì§ˆë¬¸ ì˜ì—­ */}
          <div className="lg:col-span-2">
            <Card className="shadow-2xl border-0 bg-white/95 backdrop-blur overflow-hidden h-full">
              <CardContent className="p-8">
                {/* ì£¼ì œ ì œëª© */}
                <div className="text-center mb-8">
                  <div className="inline-flex items-center gap-3 bg-orange-100 text-orange-700 px-4 py-2 rounded-full mb-4">
                    <BookOpen className="w-5 h-5" />
                    <span className="font-medium">
                      ì£¼ì œ {currentTopic + 1}/{topics.length}
                    </span>
                  </div>
                  <div className="text-6xl mb-4">{topics[currentTopic].icon}</div>
                  <h2 className="text-3xl font-bold text-gray-800 mb-4">{topics[currentTopic].title}</h2>
                </div>

                {/* ì§ˆë¬¸ ë‚´ìš© */}
                <div className="bg-gradient-to-r from-orange-50 to-red-50 p-8 rounded-2xl mb-8">
                  <div className="flex items-start gap-4">
                    <div className="w-12 h-12 bg-orange-500 rounded-full flex items-center justify-center flex-shrink-0">
                      <BookOpen className="w-6 h-6 text-white" />
                    </div>
                    <div className="flex-1">
                      <p className="text-sm text-orange-600 font-medium mb-2">ì§ˆë¬¸ì„ ì½ì–´ì£¼ì„¸ìš”</p>
                      <p className="text-xl leading-relaxed text-gray-800 whitespace-pre-line">
                        {topics[currentTopic].question}
                      </p>
                    </div>
                  </div>
                </div>
                {/* ë…¹ìŒ ìƒíƒœ */}
                {isRecording && (
                  <div className="mb-8">
                    <div className="bg-red-50 border border-red-200 rounded-lg p-6">
                      <div className="flex items-center justify-between">
                        <div className="flex items-center gap-3">
                          <div className="w-4 h-4 bg-red-500 rounded-full animate-pulse"></div>
                          <div>
                            <p className="font-medium text-red-800">ë…¹ìŒ ì¤‘...</p>
                            <p className="text-sm text-red-600">ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”</p>
                          </div>
                        </div>
                        <div className="flex items-center gap-2">
                          <div className="w-2 h-2 bg-red-500 rounded-full animate-pulse"></div>
                          <div className="w-2 h-2 bg-red-500 rounded-full animate-pulse" style={{ animationDelay: '0.2s' }}></div>
                          <div className="w-2 h-2 bg-red-500 rounded-full animate-pulse" style={{ animationDelay: '0.4s' }}></div>
                        </div>
                      </div>
                      
                      {/* ì˜¤ë””ì˜¤ ë ˆë²¨ í‘œì‹œ */}
                      <div className="mt-4">
                        <div className="flex items-center gap-1 h-8">
                          {Array.from({ length: 20 }, (_, i) => (
                            <div
                              key={i}
                              className="flex-1 bg-red-200 rounded-sm transition-all duration-100"
                              style={{
                                height: `${Math.max(10, (audioLevel / 255) * 100 * (i + 1) / 20)}%`,
                                backgroundColor: audioLevel > 50 ? '#ef4444' : '#fecaca'
                              }}
                            />
                          ))}
                        </div>
                      </div>
                    </div>
                  </div>
                )}

                {/* ë…¹í™”ëœ ë¯¸ë””ì–´ ì¬ìƒ */}
                {recordedMedia && !isRecording && (
                  <div className="mb-8">
                    <div className="bg-green-50 border border-green-200 rounded-lg p-6">
                      <div className="flex items-center justify-between">
                        <div className="flex items-center gap-3">
                          <CheckCircle className="w-6 h-6 text-green-600" />
                          <div>
                            <p className="font-medium text-green-800">ë‹µë³€ì´ ë…¹í™”ë˜ì—ˆìŠµë‹ˆë‹¤</p>
                            <p className="text-sm text-green-600">ì•„ë˜ì—ì„œ ë‹¤ì‹œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤</p>
                          </div>
                        </div>
                        <video controls src={recordedMedia} className="w-100 h-31 rounded-lg" />
                      </div>
                    </div>
                  </div>
                )}

                {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ë“¤ */}
                <div className="flex items-center justify-center gap-4">
                  <Button
                    onClick={replayQuestion}
                    variant="outline"
                    className="h-12 px-6 border-orange-300 text-orange-600 hover:bg-orange-50 bg-transparent"
                  >
                    <RotateCcw className="w-4 h-4 mr-2" />
                    ë‹¤ì‹œì¬ìƒ
                  </Button>

                  <Button
                    onClick={isRecording ? stopRecording : () => startRecording(false)}
                    className={`h-12 px-8 font-medium ${
                      isRecording
                        ? "bg-red-500 hover:bg-red-600 text-white"
                        : "bg-green-500 hover:bg-green-600 text-white"
                    }`}
                  >
                    {isRecording ? (
                      <>
                        <MicOff className="w-5 h-5 mr-2" />
                        ë…¹ìŒ ì¤‘ì§€
                      </>
                    ) : (
                      <>
                        <Mic className="w-5 h-5 mr-2" />
                        ìˆ˜ë™ ë…¹ìŒ
                      </>
                    )}
                  </Button>

                  <Button
                    onClick={handleNext}
                    disabled={!recordedMedia && !isRecording}
                    className="h-12 px-6 bg-orange-500 hover:bg-orange-600 text-white"
                  >
                    {currentTopic === topics.length - 1 ? 'ì™„ë£Œí•˜ê¸°' : 'ë‹¤ìŒ ì£¼ì œ'}
                    <ArrowRight className="w-4 h-4 ml-2" />
                  </Button>
                </div>
              </CardContent>
            </Card>
          </div>

          {/* ì˜¤ë¥¸ìª½: ì‚¬ìš©ì ìº  í™”ë©´ */}
          <div className="lg:col-span-1">
            <Card className="shadow-2xl border-0 bg-white/95 backdrop-blur overflow-hidden h-full">
              <CardContent className="p-6">
                <div className="space-y-4">
                  <WebcamView 
                    isRecording={isRecording}
                    onStreamReady={setWebcamStream}
                    videoRef={videoRef}
                  />
                  
                  {/* ê°ì • ë¶„ì„ ê²°ê³¼ */}
                  <div className="bg-white/80 backdrop-blur rounded-lg p-4">
                    <h3 className="text-lg font-semibold mb-2">ê°ì • ë¶„ì„</h3>
                    <div className="space-y-2">
                      <div className="flex justify-between items-center">
                        <span>í˜„ì¬ ê°ì •:</span>
                        <span className="font-medium text-orange-600">{emotion}</span>
                      </div>
                      <div className="flex justify-between items-center">
                        <span>ì‹ ë¢°ë„:</span>
                        <span className="font-medium text-orange-600">{Math.round(confidence * 100)}%</span>
                      </div>
                      <div className="w-full bg-gray-200 rounded-full h-2">
                        <div
                          className="bg-orange-600 h-2 rounded-full transition-all duration-300"
                          style={{ width: `${confidence * 100}%` }}
                        />
                      </div>
                    </div>
                  </div>
                </div>
              </CardContent>
            </Card>
          </div>
        </div>
      </div>
    </div>
  )
} 